{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ncl-VBUTZrAK"
      },
      "outputs": [],
      "source": [
        "import configparser\n",
        "import os\n",
        "\n",
        "config = configparser.RawConfigParser()\n",
        "config.read('keys.config')\n",
        "os.environ[\"ACTIVELOOP_TOKEN\"] = config.get('keys', 'active_loop_key')\n",
        "os.environ[\"OPENAI_API_KEY\"] = config.get('keys', 'open_ai_key')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rrifxya9ZxdY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.10/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.11) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "# Load Sample\n",
        "f = open('sample_espresso.txt', 'r')\n",
        "rows = f.readlines()\n",
        "\n",
        "sample = []\n",
        "for r  in rows:\n",
        "    if \"Category:\" in r: # New Category\n",
        "        sample.append({\"query\": r})\n",
        "    else:\n",
        "        if sample[-1].get(\"answer\", None) is None:\n",
        "            sample[-1][\"answer\"] = \"\"\n",
        "        \n",
        "        sample[-1][\"answer\"] += r\n",
        "\n",
        "# create an example template\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# create a prompt example from above template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "# now break our previous prompt into a prefix and suffix\n",
        "# the prefix is our instructions\n",
        "prefix = \"\"\"The following is a template for Energy-related News, grouped by category.\\\n",
        "    Use this template as a sample to generate new Energy-related News by category.\n",
        "\"\"\"\n",
        "# and the suffix our user input and output indicator\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "# now create the few-shot prompt template\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=sample,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6mfosAQMahBU",
        "outputId": "eed438c8-2f2a-41c2-a65b-cb00aa193bb9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"1. OPEC members convene for critical meeting to discuss global energy policies and market stability.\\n2. Iran's enriched uranium levels rise as international inspectors report reinstallation of monitoring equipment.\\n3. Geopolitical tensions rise as OPEC meeting coincides with Iran's nuclear developments.\""
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import LLMChain\n",
        "\n",
        "# load the model\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=1.0)\n",
        "\n",
        "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
        "chain.run(\"Create 3 new headlines for Category Geopolitics?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
