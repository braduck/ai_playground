{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ncl-VBUTZrAK"
      },
      "outputs": [],
      "source": [
        "import configparser\n",
        "import os\n",
        "\n",
        "config = configparser.RawConfigParser()\n",
        "config.read('../keys.config')\n",
        "os.environ[\"ACTIVELOOP_TOKEN\"] = config.get('keys', 'active_loop_key')\n",
        "os.environ[\"OPENAI_API_KEY\"] = config.get('keys', 'open_ai_key')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Rrifxya9ZxdY"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "# create our examples\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"What's the weather like?\",\n",
        "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
        "    }, {\n",
        "        \"query\": \"How old are you?\",\n",
        "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# create an example template\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# create a prompt example from above template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "# now break our previous prompt into a prefix and suffix\n",
        "# the prefix is our instructions\n",
        "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
        "assistant. The assistant is known for its humor and wit, providing\n",
        "entertaining and amusing responses to users' questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "# and the suffix our user input and output indicator\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "# now create the few-shot prompt template\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6mfosAQMahBU",
        "outputId": "eed438c8-2f2a-41c2-a65b-cb00aa193bb9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'To find the perfect balance between pizza and ice cream.'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import LLMChain\n",
        "\n",
        "# load the model\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
        "\n",
        "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
        "chain.run(\"What's the meaning of life?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
